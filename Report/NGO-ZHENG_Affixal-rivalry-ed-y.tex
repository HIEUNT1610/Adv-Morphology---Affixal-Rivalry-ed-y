\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{natbib}
\usepackage{amsmath}


\title{A distributional analysis 

of affixal rivalry between \emph{-ed} and \emph{-y} in English}
\author{Ngo Trung Hieu & Zheng Peng \thanks{M1 Linguistique Informatique - Université Paris Cité}}
\bibliographystyle{plainnat}
\date{}

\begin{document}
\maketitle

\begin{abstract}
    The paper \emph{Affixal rivalry and its purely semantic resolution among English derived adjectives} by Nagano (2022) has claimed an interesting finding about the affixal rivalry between the ending -ed and -y in English. According to Nagano, there exists a purely semantically differentiated affixal rivalry between these two endings in the denominalized adjectives, created from English concrete nouns. The authors of this study are interested in looking at this finding from a different perspective, by using the distributional methodology, as demonstrated in Naranjo and Bonami (2023).
\end{abstract}

\section{Literature review}
\subsection{Affixal rivalry in English affixes \emph{-ed} and \emph{-y}}
Nagano (2022) claimed that there exists an affixal rivalry between the two adjectivalizing suffixes \emph{-ed} and \emph{-y} that differentiated purely on semantics, and looked at the both the case of deverbal and denominal adjectivalization using the traditional method of doublet comparison. In the case of denominalized adjectives, Naganon found that denominalization from concrete nouns had to conform to the Natural Origin constrain, which requires a natural relationship between the head noun and the base noun in derived adjectives. This constrain led to a difference in scale type of the derived adjectives. The deverbalized adjectives were found to be double differentiated, as the certain input base were preferred and the output scale were different. These findings are interesting, and this study aims to look at the claim of affixal rivalry between \emph{-ed} and \emph{-y} in English nouns, and the claim of the semantic difference between the two affixal processes from a distributional approach.

\subsection{Distributional methodology for affixal rivalry}
Naranjo and Bonami (2023) discussed the concept of morphological rivalry, which refers to the competition between different word formation processes that can create similar or related words. The authors proposed using distributional semantics, which is a computational approach to studying meaning based on the distribution of words in a corpus, to assess the semantic similarity between different word formation processes. The difference vectors between pairs of words that are related by a particular process, such as affixation or conversion, can be used to represent the semantic effect of that process. From that line of thought, by comparing the average difference vectors for different processes, the similarity between those processes are in terms of their semantic impact can be assessed.

The paper presents two experiments that use this approach to identify rival derivational processes in French morphology. The first experiment involved calculating the difference vectors of different affixal processes and compared them to each other to find potential affixal rivalries. The second experiment used Boosting Trees classifiers to predict the process that relates a base and a derived lexeme, with the hypothesis that if the classifier is unable to correctly predict a derivation of a base lexeme, there exists affixal rivalry. This study attempts to apply the methodology from Naranjo and Bonami (2023) paper to analyze the affixal rivalry between \emph{-ed} and \emph{-y}. 

\subsection{Semantic retrofitting for word embeddings}
Faruqui et al. (2015) proposed a method called \emph{retrofitting} for improving the quality of word vectors by incorporating semantic information from external resources, such as WordNet or FrameNet. Word vectors are numerical representations of words that capture their distributional properties, and these vectors are often used in various natural language processing tasks such as sentiment analysis, machine translation, and named entity recognition.

The retrofitting method involves adjusting the word vectors to better fit the semantic relationships in the external resources. Specifically, the method encourages linked words to have similar vector representations. The authors show that retrofitting can be applied to pre-trained word vectors obtained using any vector training model and gives consistent improvement in performance on evaluation benchmarks with different word vector lengths. The method can also be applied to multiple languages. The authors evaluated the method on several tasks, including word similarity, syntactic relations, and sentiment analysis, and show that retrofitting improves performance on most tasks. They also compare retrofitting to using semantic lexicons during vector training and show that retrofitting is competitive with this approach. For this reason, this study attempts to apply this method of retrofitting to improve semantic properties of word vectors, in order to prove that the two affixal processes of \emph{-ed} and \emph{-y} are indeed semantically related.


\section{Research questions}
    Because Nagano (2022) explained the affixal rivalry between \emph{-ed} and \emph{-y} in English from  qualitative approach, the dataset used in the paper is very limited. We are interested in investigating this finding from a distributional perspective. Therefore, the research questions of this study are as follows:
    \begin{itemize}
        \item Is there similarity between the affixes \emph{-ed} and \emph{-y} in terms of their distributional behavior? To answer this question, this study will look at the distributional representations of the two affixes, by using conventional distributional word embeddings such as word2vec.
        \item Are there evidences of affixal rivalry between \emph{-ed} and \emph{-y} in English, in a larger and not hand-picked dataset? To answer this question, this study will train a neural network on the dataset from Nagano (2022), and test the classifier on unseen data extracted from UniMorph. If there exists a rivalry between the two affixes, the classifier should not be able to accurately predict the correct affix for the unseen data.
        \item The conventional distributional representations of words may not be able to fully capture semantic relationships between words. Therefore, this study will also implement a retrofitting algorithm from Faruqui (2015) to augment the word vectors. If the similarity between the two affixes is changed after retrofitting, it means that there exists a certain semantic difference between the two affixes.
    \end{itemize}

\section{Methodology and Experiment Design}
There are three experiments in this project. To answer the first research question, we calculated and compared the difference vectors between the two affixes to see if they are semantically comparable or not. The second experiment is to train a neural network classifier on these difference vectors, then test the classifier on unseen data, in this case, the data from UniMorph. The last question of augmenting the semantics of the word vectors using the semantic retrofitting method was addressed in the third experiment.

\subsection{Experiment 1:}
    For this experiment, the dataset from Nagano (2022) and a distributional word vectorization system are needed. The idea is to represent the roots of chosen words and their derivations in a vector space so that we can use computation to calculate the similarity. The two affixal processes are seen as semantically comparable if the distributional vectors of the processes are cosine similar. Because cosine similarity is in the range of [-1, 1] with -1 as totally different and 1 as totally similar, we believe that a similarity of around 0.5 would suffice to prove that the two affixal processes are semantically similar. The 76 examples given by Nagano are first converted to vectors by using the Google News 300 pre-trained Word2Vec model. Then the difference vectors are calculated using the roots and their derivations. Finally, the difference vectors of the two affixal processes are averaged, then compared to each other using cosine similarity. If the two affixes are distributionally similar, the difference vectors should be similar as well.

\subsection{Experiment 2:}
    For the second experiment, we will create a neural network classifier with one hidden layer to train on the Nagano dataset, then use the classifier to predict the derivation of the unseen root words during training. The training data from Nagano is converted into a set of roots and labels. The roots are labeled \emph{0} if having a \emph{ed} derivation, and \emph{1} if \emph{-y}. The lexemes are then vectorized, then the training data is fed into the classifier. The test dataset is extracted from UniMorph, and the words are filtered to only include words that can have \emph{-ed} or \emph{-y} affixes. The classifier will be trained on the vectorized base lexemes and their derivations from the Nagano dataset, and the classifier will be trained for 100 epochs using 10 folds to maximize learning efficiency due to the limited number of training examples. After training, the classifier will be tested on the UniMorph dataset of 4191 words, which were then vectorized by the word2vec model. The ratio of \emph{-ed} affix is 42.92\%, while the ratio of \emph{-y} affix is 57.08\%. The classifier will be tested on this UniMorph dataset. The hypothesis is if the classifier can predict the derivation of the unseen words with an accuracy of around 50\%, it means that the classifier is not able to distinguish between the two affixes, and therefore there exists an affixal rivalry between the two affixes.

\subsection{Experiment 3:}
    Semantic retrofitting is a method to improve the quality of the word vectors by using the semantic information (synonyms, antonyms, hyper/hyponyms) from a lexicon. 

    In this experiment, we will use the retrofitting method from Faruqui et al. (2015) and the WordNet lexicon to improve the quality of the word vectors from the training data. The WordNet lexicon provides the semantic information for the words, and then uses the retrofitting method to create a new word vector based on the old word vector and the ``neighbors'' of the word. The idea of retrofitting is to have a new word vector that is close (has a better cosine similarity) to the old word vector, but is also closer to the neighbors of the word. This new word vector is then said to be capturing the semantic information of the word better than the purely distributional word vector.

    Then we will recalculate the difference vectors to see if there are improvements or not, and if these improvements are significant. If the difference vectors are changed after retrofitting, it means that the two affixes might be semantically differentiated.

\section{Results}
    \subsection{Experiment 1:}
    Using the word2vec model, we first calculate the average similarity between two random words in the lexicon. To do this, we randomly chose 100,000 pairs from the lexicon, and compute the cosine similarity between the pairs. The result of this computation is 0.1308, and this serves as a baseline for the cosine similarity results. 

    The next step is to calculate the average difference vectors between the roots and the derivations of the Nagano dataset. The vectorization model is used on both the bases and the derivations of the words in the Nagano dataset, and then the difference vector is calculated as $\text{difference vector}$ = $\text{root vector}$ - $\text{derivation vector}$. The difference vectors between each base lexeme and its two derivations are then compared using cosine similarity, then these values were used to calculate the average similarity between the two affixes \emph{-ed} and \emph{-y}. The result of this computation is 0.4162, which is 3.18 times higher than the baseline. This result shows that the two affixes are distributionally similar, signalling that the two affixes can be semantical rivals. The detailed result of this experiment is shown in Table 1.

    \begin{table}[p]
    \begin{center}
        \begin{tabular}{||c c||}
        \hline
        ROOT & COSINE\_SIMILARITY \\
        \hline
        \hline
        leg & 0.526988 \\
        bone & 0.602122 \\
        price & 0.537156 \\
        wit & 0.264876 \\
        curve & 0.628152 \\
        head & 0.354714 \\
        brain & 0.603593 \\
        cheek & 0.448141 \\
        feather & 0.641470 \\
        hand & 0.227643 \\
        head & 0.354714 \\
        hip & 0.482956 \\
        mouth & 0.506514 \\
        nose & 0.364511 \\
        skin & 0.517622 \\
        tooth & 0.620628 \\
        edge & 0.395138 \\
        loft & 0.324167 \\
        room & 0.246554 \\
        shape & 0.319353 \\
        taste & 0.323992 \\
        fish & 0.202100 \\
        leaf & 0.354984 \\
        rock & 0.445634 \\
        sand & 0.314394 \\
        stone & 0.345706 \\
        water & 0.352573 \\
        air & 0.181804 \\
        cloud & 0.606249 \\
        dust & 0.459283 \\
        ice & 0.322265 \\
        mist & 0.297710 \\
        snow & 0.462827 \\
        sun & 0.424234 \\
        dress & 0.036261 \\
        oil & 0.509179 \\
        spice & 0.556888 \\
        sugar & 0.652546 \\
        \hline    
            
        \end{tabular}
    \end{center}
    \caption{ Cosine similarity between the derivations of each base lexeme}
    \end{table}


    \subsection{Experiment 2:}
    First, the test data was extracted from the UniMorph database, only finding the denominalized adjectives ending in \emph{-ed} and \emph{-y}. This results in a list of 4191 words, with 42.92\% ending in \emph{-ed} and 57.08\% ending in \emph{-y}. 

    Next, a neural network classifier was initialized using PyTorch. This classifier has one hidden layer of 128 neurons, using ReLU as the activation function. The input layer has 300 neurons, which is the same as the dimension of the word vectors. The output layer has 2 neurons, which is the number of labels for prediction.  
    
    The classifier was then trained on the Nagano dataset of 76 examples. The classifier was trained for 100 epochs, using 10 folds to maximize learning efficiency. When tested on the test dataset from UniMorph, the classifier gave an accuracy of 49.146\%, which is slightly lower than the expected result of 50\%. This result shows that the classifier trained on the difference vectors of \emph{-ed} and \emph{-y} is not able to distinguish between the two affixes, and therefore distributionally, there exists an affixal rivalry between these two affixes. The detailed result of this experiment is shown in the attached Jupyter notebook. However, this result is still lower than the most frequent class of 57.08\%, so there might be room to improve. 

    \subsection{Experiment 3:}
    For the last experiment, we wanted to semantically retrofit the word vectors to see if there is semantic similarity between \emph{-ed} and \emph{-y} or not. A lexicon created from WordNet was used to provide the semantic information for the words. The retrofitting method from Faruqui et al. (2015) was used to create a new word vector based on the old word vector and the semantic information from the lexicon. The new word vector is calculated as follows: $\text{new word vector}$ = $\alpha$ * $\text{old word vector}$ + $\sum_{i=1}^{d_i}$ $\frac{1}{d_i}$ * ($\text{neighbor vectors})$, where $\alpha$ is a hyperparameter and $d_i$ is the number of neighbors of the word. The value of $\alpha$ is set to 1. This algorithm allows the new word vector to not only be similar to the old word vector, but also more similar to the semantically related neighbors of the word.

    The retrofitting method was applied to the word vectors of the Nagano dataset, and then the difference vectors were recalculated. The average similarity between the two affixes was then calculated again, and the result is 0.495. Compared to the original vectors, the average similarity was increased only by a small amount of 0.0788, but it is still 3.19 times higher than the baseline. This modest gain was due to the fact that a few of the similarities of the word vectors and their derivations were increased substantially, such as \emph{bone} from 0.602 to 0.998 or \emph{curve} from 0.628 to 0.997, but many others were decreased. Out of 30 examples that had the cosine similarities changed by the retrofitting algorithm, there were 15 examples with increased similarities and 15 others with decreased similarities. The average similarity was increased due to some substantial increases, but this result could be seen as an evidence for the two affixal processes to be semantically differentiated. The detailed result of this experiment is shown in Table 2.

    \begin{table}[p]
        \begin{center}
            \begin{tabular}{||c c c c c||}
            \hline
            ROOT & DERIV\_ed & DERIV\_y & NEW\_COSINE\_SIMILARITY & OLD\_COSINE\_SIMILARITY \\
            \hline
            \hline
            bone & boned & boney & 0.998281 & 0.602122 \\
            curve & curved & curvy & 0.997423 & 0.628152 \\
            head & headed & heady & 0.996630 & 0.354714 \\
            brain & brained & brainy & 0.996457 & 0.603593 \\
            hip & hipped & hippy & 0.482956 & 0.354714 \\
            mouth & mouthed & mouthy & 0.506514 & 0.482956 \\
            nose & nosed & nosy & 0.364511 & 0.506514 \\
            skin & skinned & skinny & 0.517622 & 0.364511 \\
            tooth & toothed & toothy & 0.998461 & 0.517622 \\
            edge & edged & edgy & 0.395138 & 0.620628 \\
            loft & lofted & lofty & 0.324167 & 0.395138 \\
            room & roomed & roomy & 0.246554 & 0.324167 \\
            shape & shaped & shapely & 0.996835 & 0.246554 \\
            taste & tasted & tasty & 0.323992 & 0.319353 \\
            fish & fished & fishy & 0.202100 & 0.323992 \\
            leaf & leafed & leafy & 0.354984 & 0.202100 \\
            rock & rocked & rocky & 0.445634 & 0.354984 \\
            sand & sanded & sandy & 0.314394 & 0.445634 \\
            stone & stoned & stony & 0.345706 & 0.314394 \\
            water & watered & watery & 0.352573 & 0.345706 \\
            air & aired & airy & 0.181804 & 0.352573 \\
            cloud & clouded & cloudy & 0.606249 & 0.181804 \\
            dust & dusted & dusty & 0.459283 & 0.606249 \\
            ice & iced & icy & 0.322265 & 0.459283 \\
            mist & misted & misty & 0.297710 & 0.322265 \\
            snow & snowed & snowy & 0.462827 & 0.297710 \\
            sun & sunned & sunny & 0.424234 & 0.462827 \\
            dress & dressed & dressy & 0.036261 & 0.424234 \\
            oil & oiled & oily & 0.509179 & 0.036261 \\
            spice & spiced & spicy & 0.556888 & 0.509179 \\
            sugar & sugared & sugary & 0.652546 & 0.556888 \\
            \hline    
                
            \end{tabular}
        \end{center}
        \caption{Retrofitted and original cosine similarities between the derivations of each base lexeme.}
        \end{table}
\section{Discussion}
    The results of the three experiments show that there exists both distributional and semantic similarity between the two affixes \emph{-ed} and \emph{-y}. However, the accuracy of the second experiment is still much lower than the baseline of predicting the most frequent affix, which is 57.08\%. We looked at this problem from a few other perspectives:
    \begin{itemize}
        \item The classifier might be learning too many unnecessary details from the train set due to the power of neural networks, therefore cannot generalize well to the test set. To solve this problem, we implemented an early stopping mechanism to stop the training process when the test accuracy stops improving. With the early stopping mechanism implemented, the classifier had the accuracy for the train set at 100\% in just one epoch, and the training was stopped because the accuracy for the test set was decreased in the next epoch. The accuracy for the test set was 56.662\%, which is much higher than the previous result but still a little lower than the most frequent baseline. This result showed that the classifier is indeed learning too many unnecessary details from the train set, and the early stopping mechanism is able to solve this problem. 
        \item The training set might be too small for the classifier to learn the difference between the two affixes. To solve this problem, we reversed the train set and test set, so that the train set is substantially larger than the test set. After the reversal, the accuracy of the prediction on the Nagano set was at exactly 50\%, which is the baseline for this dataset. This result showed that the datasets were not the problem. 
        \item The train set and the test set might be too different from each other for the classifier to work, or the classifier is not efficient enough to capture information. To check this issue, we implemented the classifier on another task of predicting if a word is in singular or plural form. The data was extracted from UniMorph, and split into a train and test set with a ratio of 80:20. After training, the classifier achieved an accuracy score of 45.45\% on the test set, which is a little lower than the baseline of 50\%. This result showed that the classifier is not efficient enough to capture information, and the train set and test set are not too different from each other.
        
    \end{itemize}
    After looking closer into the problem from other perspectives, none of the above is relevant. From this, we can conclude that there is indeed affixal rivalry between \emph{-ed} and \emph{-y}, and the two affixes are also semantically similar, but also can be semantically differentiated.
\section{Conclusion}    
    This study attempted to distributionally prove the findings from Nagano's paper in 2022, that there exists a semantically affixal rivalry between the affixes \emph{-ed} and \emph{-y}. The distributional difference vectors of the two affixal processes are cosine similar, therefore the two processes are semantically similar. However, retrofitting these vectors with semantic information managed to slightly improve the similarity, and there were as many improvements as reductions, so these processes can be argued to be semantically differentiated as well. The neural network classifier managed to achieve an accuracy of 56.662\% on the UniMorph dataset, slightly lower than the most frequent baseline, which translates to the fact that a classifier cannot distinguish between the two affixes. Therefore, there exists an affixal rivalry between the two affixes. From these experiments, we can conclude the findings of Nagano in 2022, that indeed there exists an affixal rivalry between the affixes \emph{-ed} and \emph{-y}, and these two processes can be semantically differentiated. 
\nocite{*}
\bibliography{biblio}

\end{document}